{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1: Customer Database\n",
    "**This is the first of three mandatory projects to be handed in as part of the assessment for the course 02807 Computational Tools for Data Science at Technical University of Denmark, autumn 2019.**\n",
    "\n",
    "#### Practical info\n",
    "- **The project is to be done in groups of at most 3 students**\n",
    "- **Each group has to hand in _one_ Jupyter notebook (this notebook) with their solution**\n",
    "- **The hand-in of the notebook is due 2019-10-13, 23:59 on DTU Inside**\n",
    "\n",
    "#### Your solution\n",
    "- **Your solution should be in Python**\n",
    "- **For each question you may use as many cells for your solution as you like**\n",
    "- **You should document your solution and explain the choices you've made (for example by using multiple cells and use Markdown to assist the reader of the notebook)**\n",
    "- **You should not remove the problem statements, and you should not modify the structure of the notebook**\n",
    "- **Your notebook should be runnable, i.e., clicking [>>] in Jupyter should generate the result that you want to be assessed**\n",
    "- **You are not expected to use machine learning to solve any of the exercises**\n",
    "- **You will be assessed according to correctness and readability of your code, choice of solution, choice of tools and libraries, and documentation of your solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Your team has been hired by the company X as data scientists. X makes gadgets for a wide range of industrial and commercial clients.\n",
    "\n",
    "As in-house data scientists, your teams first task, as per request from your new boss, is to optimize business operations. You have decided that a good first step would be to analyze the companys historical sales data to gain a better understanding of where profit is coming from. It may also reveal some low hanging fruit in terms of business opportunities.\n",
    "\n",
    "To get started, you have called the IT department to get access to the customer and sales transactions database. To your horror you've been told that such a database doens't exist, and the only record of sales transactions is kept by John from finance in an Excel spreadsheet. So you've emailed John asking for a CSV dump of the spreadsheet...\n",
    "\n",
    "In this project you need to clean the data you got from John, enrich it with further data, prepare a database for the data, and do some data analysis. The project is comprised of five parts. They are intended to be solved in the order they appear, but it is highly recommended that you read through all of them and devise an overall strategy before you start implementing anything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Cleaning the data\n",
    "John has emailed you the following link to the CSV dump you requested.\n",
    "\n",
    "- [transactions.csv](https://raw.githubusercontent.com/patrickcording/02807-comp-tools/master/docker/work/data/transactions.csv)\n",
    "\n",
    "It seems as though he has been a bit sloppy when keeping the records. \n",
    "\n",
    "In this part you should:\n",
    "- Explain what the data is\n",
    "- Clean it to prepare it for inserting into a database and doing data analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start off by fetching the data, checking the data types and the first few lines to get a general feeling for what we are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('transactions.csv')\n",
    "\n",
    "print(df.dtypes)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data consists of:\n",
    "- Part serial number\n",
    "- Company name\n",
    "- Company location, country\n",
    "- Company location, city\n",
    "- Price of the part\n",
    "- Date of transaction\n",
    "\n",
    "Apparently there are some typos in a couple of dates that need to be fixed before they can be converted to datetime format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip this cell unless you want to see the try-except in action\n",
    "for i in range(len(df)):\n",
    "    try:\n",
    "        pd.to_datetime(df.date[i])\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Incorrect data format: \", df.date[i], ' row: ', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ValueError: ('Incorrect data format: ', '2016-06-32 07:22:28', ' row: ', 3539)\n",
    "# ValueError: ('Incorrect data format: ', '2016-06-32 08:08:48', ' row: ', 3540)\n",
    "df.date[3539] = '2016-06-30 07:22:28'\n",
    "df.date[3540] = '2016-06-30 08:08:48'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then check how many unique values we have in each column and whether there are any missing values or typos. We also put date into a proper datetime format in order to get earliest and latest date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('number of rows: ', len(df), '\\n')\n",
    "print('unique parts: ', len(df.part.unique()))\n",
    "print('nan: ', df.part.isna().sum(), '\\n')\n",
    "\n",
    "print(df.country.unique())\n",
    "print('nan: ', df.country.isna().sum(), '\\n')\n",
    "\n",
    "print(df.city.unique())\n",
    "print('nan: ', df.city.isna().sum(), '\\n')\n",
    "\n",
    "print(df.company.unique())\n",
    "print('company nan: ', df.company.isna().sum(), '\\n')\n",
    "\n",
    "print('price nan: ', df.price.isna().sum())\n",
    "print('date nan: ', df.date.isna().sum(), '\\n')\n",
    "\n",
    "print('price in €: ', len(df[df['price'].str.contains(\"€\")==True]))\n",
    "print('price in £: ', len(df[df['price'].str.contains(\"£\")==True]))\n",
    "print('price in $: ', len(df[df['price'].str.contains(\"$\")==True]))\n",
    "print('price in ¥: ', len(df[df['price'].str.contains(\"¥\")==True]))\n",
    "print('undefined price: ', len(df[df['price'].str.contains(\"[£$¥€]\")==False]), '\\n')\n",
    "\n",
    "df['datetime'] = pd.to_datetime(df.date)\n",
    "print('date min: ', df.datetime.min())\n",
    "print('date max: ', df.datetime.max())\n",
    "df.drop(['date'], inplace=True, axis=1)\n",
    "\n",
    "df[df['price'].str.contains(\"[£$¥€]\")==False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Issues with data:\n",
    "- Missing values in part, country, city and price **✔**-ish (What should we do with part?)\n",
    "- Typos in country and city **✔**\n",
    "- Germany/Tyskland, United States/US, Portugal/Portuga **✔**\n",
    "- Typos in date (already fixed) **✔**\n",
    "- Prices are in different currencies **✔**\n",
    "- Some prices unidentified **✔**-ish (Some negative prices)\n",
    "- Typos and nonsense company names **✔**\n",
    "\n",
    "#### Cleanup:\n",
    "Lets start with fixing typos in company, country and city. The correct currency for four rows can be deduced from country/city. Rows with missing company and/or price are removed since without that the data is of very limited use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc = df # df cleaned\n",
    "\n",
    "dfc['company'] = df['company'].replace({'Laj0':'Lajo','Zooxo.':'Zooxo','Thoughtmixz':'Thoughtmix'})\n",
    "dfc['country'] = df['country'].replace({'Tyskland':'Germany', 'US':'United States', 'Portuga':'Portugal'})\n",
    "dfc['city'] = df['city'].replace({'Amadora\\t':'Amadora'})\n",
    "\n",
    "dfc = dfc[dfc.company != ' -']\n",
    "dfc = dfc[dfc.company != ' a']\n",
    "dfc = dfc[dfc.company != 'aa']\n",
    "\n",
    "dfc = dfc[dfc.price != '-']\n",
    "dfc = dfc[dfc.price != 'void']\n",
    "dfc = dfc[dfc.price != 'na']\n",
    "dfc = dfc[dfc.price.isna()==False]\n",
    "\n",
    "dfc.iloc[[2414],[4]] = dfc.iloc[[2414],[4]] + '€'\n",
    "dfc.iloc[[2415],[4]] = dfc.iloc[[2415],[4]] + '€'\n",
    "dfc.iloc[[2526],[4]] = dfc.iloc[[2526],[4]] + '€'\n",
    "dfc.iloc[[2528],[4]] = dfc.iloc[[2528],[4]] + '€'\n",
    "\n",
    "# dfc[dfc['price'].str.contains(\"[£$¥€]\")==False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The missing city values are recovered by comparing rows with the same company and the missing country values are likewise deduced from the city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc['city'] = dfc['city'].fillna(dfc['company'].map({\n",
    "    'Brainsphere':'Braga', \n",
    "    'Shufflebeat':'Porto',\n",
    "    'Ntags':'Lisbon',\n",
    "    'Thoughtmix':'Amadora',\n",
    "    'Yozio':'Patras',\n",
    "    'Twitterbeat':'Annecy',\n",
    "    'Zooxo':'London',\n",
    "    'Zoonder':'Boston',\n",
    "    'Teklist':'Arnhem',\n",
    "    'Wordify':'New York',\n",
    "    'Kanoodle':'Niihama' \n",
    "}))\n",
    "\n",
    "dfc['country'] = dfc['country'].fillna(dfc['city'].map({\n",
    "    'Arnhem':'Netherlands',\n",
    "    'Braga':'Portugal',\n",
    "    'Düsseldorf':'Germany',\n",
    "    'Annecy':'France',\n",
    "    'Heraklion':'Greece',\n",
    "    'Barcelona':'Spain',\n",
    "    'Lisbon':'Portugal',\n",
    "    'Athens':'Greece',\n",
    "    'Amadora':'Portugal',\n",
    "    'Aranhas':'Portugal',\n",
    "    'New York':'United States',\n",
    "    'Patras':'Greece',\n",
    "    'Amiens':'France',\n",
    "    'Almada':'Portugal',\n",
    "    'Boston':'United States',\n",
    "    'Porto':'Portugal',\n",
    "    'Paris':'France',\n",
    "    'Nice':'France',\n",
    "    'London':'United Kingdom',\n",
    "    'Niihama':'Japan',\n",
    "    'Asaka':'Japan',\n",
    "    'Nanterre':'France',\n",
    "    'Arcueil':'France',\n",
    "    'Lyon':'France',\n",
    "    'Thessaloniki':'Greece',\n",
    "    'Amsterdam':'Netherlands',\n",
    "    'Champagnole':'France',\n",
    "    'Zürich':'Germany'\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The price is then extracted and placed in a new column corresponding to the currency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc['eur'] = np.where(dfc['price'].str.contains(\"€\")==True, dfc['price'].map(lambda x: x.lstrip('€').rstrip('€')), None)\n",
    "dfc['usd'] = np.where(dfc['price'].str.contains(\"\\$\")==True, dfc['price'].map(lambda x: x.lstrip('$').rstrip('$')), None)\n",
    "dfc['gbp'] = np.where(dfc['price'].str.contains(\"£\")==True, dfc['price'].map(lambda x: x.lstrip('£').rstrip('£')), None)\n",
    "dfc['yen'] = np.where(dfc['price'].str.contains(\"¥\")==True, dfc['price'].map(lambda x: x.lstrip('¥').rstrip('¥')), None)\n",
    "\n",
    "dfc['eur'] = dfc['eur'].astype(float)\n",
    "dfc['usd'] = dfc['usd'].astype(float)\n",
    "dfc['gbp'] = dfc['gbp'].astype(float)\n",
    "dfc['yen'] = dfc['yen'].astype(float)\n",
    "\n",
    "#dfc.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chatterbridge' 'Lajo' 'Flipstorm' 'Twitterbeat' 'Voomm' 'Buzzbean'\n",
      " 'Zooxo' 'Brainsphere' 'Thoughtmix' 'Wordify' 'Teklist' 'Avaveo' 'Ntags'\n",
      " 'Innojam' 'Shufflebeat' 'Zoonder' 'Kanoodle' 'Gabcube' 'Roodel'\n",
      " 'Riffpath' 'Eimbee' 'Yozio' 'Rhycero' 'Realpoint' 'Gabtune' 'Bubblemix'\n",
      " 'Gevee' 'Tagtune' 'Ntagz'] \n",
      "\n",
      "['Spain' 'Greece' 'France' 'Germany' 'United Kingdom' 'Portugal'\n",
      " 'United States' 'Netherlands' 'Japan' 'Switzerland'] \n",
      "\n",
      "['Barcelona' 'Thessaloniki' 'Athens' 'Annecy' 'Paris' 'Düsseldorf'\n",
      " 'London' 'Braga' 'Nanterre' 'Amadora' 'New York' 'Arnhem' 'Nice' 'Lisbon'\n",
      " 'Amsterdam' 'Porto' 'Boston' 'Niihama' 'Almada' 'Aranhas' 'Heraklion'\n",
      " 'Amiens' 'Patras' 'Arcueil' 'Lyon' 'Asaka' 'Champagnole' 'Zürich'\n",
      " 'Monção' 'Vila Fria'] \n",
      "\n",
      "10\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#print(dfc.part.unique(), '\\n')\n",
    "print(dfc.company.unique(), '\\n')\n",
    "print(dfc.country.unique(), '\\n')\n",
    "print(dfc.city.unique(), '\\n')\n",
    "\n",
    "print(len(dfc[dfc.part.isna()==True]))\n",
    "print(len(dfc[dfc.company.isna()==True]))\n",
    "print(len(dfc[dfc.country.isna()==True]))\n",
    "print(len(dfc[dfc.city.isna()==True]))\n",
    "print(len(dfc[dfc.price.isna()==True]))\n",
    "print(len(dfc[dfc.datetime.isna()==True]))\n",
    "\n",
    "#dfc.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then add ID columns for part, company, country and city in preperation for the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc_part_s = dfc.sort_values(by=['part'])\n",
    "part_u = dfc_part_s.part.unique()\n",
    "df_part = pd.DataFrame({'part_id':list(range(1, len(part_u)+1)), 'name':part_u })\n",
    "\n",
    "dfc_company_s = dfc.sort_values(by=['company'])\n",
    "company_u = dfc_company_s.company.unique()\n",
    "df_company = pd.DataFrame({'company_id':list(range(1, len(company_u)+1)), 'name':company_u })\n",
    "\n",
    "dfc_country_s = dfc.sort_values(by=['country'])\n",
    "country_u = dfc_country_s.country.unique()\n",
    "df_country = pd.DataFrame({'country_id':list(range(1, len(country_u)+1)), 'name':country_u })\n",
    "\n",
    "dfc_city_s = dfc.sort_values(by=['city'])\n",
    "city_u = dfc_city_s.city.unique()\n",
    "df_city = pd.DataFrame({'city_id':list(range(1, len(city_u)+1)), 'name':city_u })\n",
    "\n",
    "dfm = dfc # df merged\n",
    "\n",
    "dfm = (dfm.merge(df_part, left_on='part', right_on='name')).drop(['name'], axis=1)\n",
    "dfm = (dfm.merge(df_company, left_on='company', right_on='name')).drop(['name'], axis=1)\n",
    "dfm = (dfm.merge(df_country, left_on='country', right_on='name')).drop(['name'], axis=1)\n",
    "dfm = (dfm.merge(df_city, left_on='city', right_on='name')).drop(['name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   country_id            name\n",
      "0           1          France\n",
      "1           2         Germany\n",
      "2           3          Greece\n",
      "3           4           Japan\n",
      "4           5     Netherlands\n",
      "5           6        Portugal\n",
      "6           7           Spain\n",
      "7           8     Switzerland\n",
      "8           9  United Kingdom\n",
      "9          10   United States\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>part</th>\n",
       "      <th>company</th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>price</th>\n",
       "      <th>datetime</th>\n",
       "      <th>eur</th>\n",
       "      <th>usd</th>\n",
       "      <th>gbp</th>\n",
       "      <th>yen</th>\n",
       "      <th>part_id</th>\n",
       "      <th>company_id</th>\n",
       "      <th>country_id</th>\n",
       "      <th>city_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54868-5165</td>\n",
       "      <td>Chatterbridge</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>784.79€</td>\n",
       "      <td>2016-01-02 00:01:05</td>\n",
       "      <td>784.79</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7402</th>\n",
       "      <td>60505-2867</td>\n",
       "      <td>Lajo</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Thessaloniki</td>\n",
       "      <td>187.99€</td>\n",
       "      <td>2016-01-02 00:05:26</td>\n",
       "      <td>187.99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5951</th>\n",
       "      <td>24385-268</td>\n",
       "      <td>Flipstorm</td>\n",
       "      <td>Greece</td>\n",
       "      <td>Athens</td>\n",
       "      <td>221.73€</td>\n",
       "      <td>2016-01-02 00:18:30</td>\n",
       "      <td>221.73</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2765</th>\n",
       "      <td>76117-001</td>\n",
       "      <td>Twitterbeat</td>\n",
       "      <td>France</td>\n",
       "      <td>Annecy</td>\n",
       "      <td>1075.82€</td>\n",
       "      <td>2016-01-02 02:32:30</td>\n",
       "      <td>1075.82</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>44946-1046</td>\n",
       "      <td>Chatterbridge</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Barcelona</td>\n",
       "      <td>412.55€</td>\n",
       "      <td>2016-01-02 04:51:55</td>\n",
       "      <td>412.55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            part        company country          city     price  \\\n",
       "0     54868-5165  Chatterbridge   Spain     Barcelona   784.79€   \n",
       "7402  60505-2867           Lajo  Greece  Thessaloniki   187.99€   \n",
       "5951   24385-268      Flipstorm  Greece        Athens   221.73€   \n",
       "2765   76117-001    Twitterbeat  France        Annecy  1075.82€   \n",
       "63    44946-1046  Chatterbridge   Spain     Barcelona   412.55€   \n",
       "\n",
       "                datetime      eur  usd  gbp  yen  part_id  company_id  \\\n",
       "0    2016-01-02 00:01:05   784.79  NaN  NaN  NaN       66           5   \n",
       "7402 2016-01-02 00:05:26   187.99  NaN  NaN  NaN       79          13   \n",
       "5951 2016-01-02 00:18:30   221.73  NaN  NaN  NaN       23           7   \n",
       "2765 2016-01-02 02:32:30  1075.82  NaN  NaN  NaN       96          24   \n",
       "63   2016-01-02 04:51:55   412.55  NaN  NaN  NaN       33           5   \n",
       "\n",
       "      country_id  city_id  \n",
       "0              7       11  \n",
       "7402           3       28  \n",
       "5951           3       10  \n",
       "2765           1        5  \n",
       "63             7       11  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_country)\n",
    "dfm.sort_values(by=['datetime']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Enriching the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common task for a data scientists is to combine or enrich data from internal sources with data available from external sources. The purpose of this can be either to fix issues with the data or to make it easier to derive insights from the data.\n",
    "\n",
    "In this part you should enrich your data with data from at least one external source. You may look to part 4 for some  inspiration as to what is required. Your solution should be automated, i.e., you can not ask the reader of your notebook to download any data manually. You should argue why and what you expect to achieve by the enrichments you are doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Solution\n",
    "\n",
    "For this part, we decided to enrich the data using the exchange rate api to fill in the gaps for the currency tables. So, what we do is detect what currency each of the records is using, and convert it to the rest of the currencies we have available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "r = requests.get('https://api.exchangeratesapi.io/latest')\n",
    "\n",
    "json = r.json()\n",
    "rates = json['rates']\n",
    "\n",
    "dfm['usd'] = np.where(dfm['eur'].isnull() != True, round(dfm['eur']/rates['USD'], 2), dfm['usd'])\n",
    "dfm['gbp'] = np.where(dfm['eur'].isnull() != True, round(dfm['eur']*rates['GBP'], 2), dfm['gbp'])\n",
    "dfm['yen'] = np.where(dfm['eur'].isnull() != True, round(dfm['eur']*rates['JPY'], 2), dfm['yen'])\n",
    "\n",
    "dfm['usd'] = np.where(dfm['gbp'].isnull() != True, round((dfm['gbp']/rates['GBP'])*rates['USD'], 2), dfm['usd'])\n",
    "dfm['eur'] = np.where(dfm['gbp'].isnull() != True, round(dfm['gbp']/rates['GBP'], 2), dfm['eur'])\n",
    "dfm['yen'] = np.where(dfm['gbp'].isnull() != True, round((dfm['gbp']/rates['GBP'])*rates['JPY'], 2), dfm['yen'])\n",
    "\n",
    "dfm['gbp'] = np.where(dfm['usd'].isnull() != True, round((dfm['usd']/rates['USD'])*rates['GBP'], 2), dfm['gbp'])\n",
    "dfm['eur'] = np.where(dfm['usd'].isnull() != True, round(dfm['usd']/rates['USD'], 2), dfm['eur'])\n",
    "dfm['yen'] = np.where(dfm['usd'].isnull() != True, round((dfm['usd']/rates['USD'])*rates['JPY'], 2), dfm['yen'])\n",
    "\n",
    "dfm['gbp'] = np.where(dfm['yen'].isnull() != True, round((dfm['yen']/rates['JPY'])*rates['GBP'], 2), dfm['gbp'])\n",
    "dfm['eur'] = np.where(dfm['yen'].isnull() != True, round(dfm['yen']/rates['JPY'], 2), dfm['eur'])\n",
    "dfm['usd'] = np.where(dfm['yen'].isnull() != True, round((dfm['yen']/rates['JPY'])*rates['USD'], 2), dfm['usd'])\n",
    "\n",
    "dfm.head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Creating a database\n",
    "Storing data in a relational database has the advantages that it is persistent, fast to query, and it will be easier access for other employees at Weyland-Yutani.\n",
    "\n",
    "In this part you should:\n",
    "- Create a database and table(s) for the data\n",
    "- Insert data into the tables\n",
    "\n",
    "You may use SQLite locally to do this. You should argue why you choose to store your data the way you do. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database is the foundation of the data and needs to be constructed so that consistency and integrity is maintained. We decided to put part, company, country and city in separate reference tables and connect them to the main table trans* with foreign keys. The most notable benefits of this are:\n",
    "\n",
    "1. Less risk of inconsistencies between rows because the data is present in only one place\n",
    "2. Less risk of incorrect data being inserted into the database because it throws an error if the correct ID does not exist in the reference tables\n",
    "\n",
    "Pandas does not seem to include a way to insert dataframes directly into existing tables and insists on creating its own using the to_sql function. To bypass this, we simply use those tables temporarily and move the data to our own.\n",
    "\n",
    "*\\*It is an established convention that table names are singlar. Transaction, however, is a reserved word in SQL, hence the stubby trans.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "engine = sqlalchemy.create_engine('sqlite:///project1.sqlite', echo=False)\n",
    "\n",
    "df_part.to_sql('part_temp', con=engine, index=False, if_exists='replace')\n",
    "df_company.to_sql('company_temp', con=engine, index=False, if_exists='replace')\n",
    "df_country.to_sql('country_temp', con=engine, index=False, if_exists='replace')\n",
    "df_city.to_sql('city_temp', con=engine, index=False, if_exists='replace')\n",
    "dfm.to_sql('trans_temp', con=engine, index=True, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect('project1.sqlite')\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute('DROP TABLE IF EXISTS trans')\n",
    "conn.commit()\n",
    "c.execute('DROP TABLE IF EXISTS part')\n",
    "conn.commit()\n",
    "c.execute('DROP TABLE IF EXISTS company')\n",
    "conn.commit()\n",
    "c.execute('DROP TABLE IF EXISTS city')\n",
    "conn.commit()\n",
    "c.execute('DROP TABLE IF EXISTS country')\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOT NULL restrictions are included to further reduce the risk of incorrect data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect('project1.sqlite')\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute('CREATE TABLE IF NOT EXISTS part(id INTEGER PRIMARY KEY, name VARCHAR(50) /*NOT NULL*/)')\n",
    "conn.commit()\n",
    "\n",
    "c.execute('CREATE TABLE IF NOT EXISTS company(id INTEGER PRIMARY KEY, name VARCHAR(50) NOT NULL)')\n",
    "conn.commit()\n",
    "\n",
    "c.execute('CREATE TABLE IF NOT EXISTS country(id INTEGER PRIMARY KEY, name VARCHAR(50) NOT NULL)')\n",
    "conn.commit()\n",
    "\n",
    "c.execute('CREATE TABLE IF NOT EXISTS city(id INTEGER PRIMARY KEY, name VARCHAR(50) NOT NULL)')\n",
    "conn.commit()\n",
    "\n",
    "query = '''\n",
    "CREATE TABLE IF NOT EXISTS trans(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    part_id INTEGER,\n",
    "    company_id INTEGER,\n",
    "    country_id INTEGER,\n",
    "    city_id INTEGER,\n",
    "    eur DECIMAL(13, 4) /*NOT NULL*/,\n",
    "    usd DECIMAL(13, 4) /*NOT NULL*/,\n",
    "    gbp DECIMAL(13, 4) /*NOT NULL*/,\n",
    "    yen DECIMAL(13, 4) /*NOT NULL*/,\n",
    "    transaction_date DATETIME NOT NULL,\n",
    "    FOREIGN KEY(part_id) REFERENCES part(id),\n",
    "    FOREIGN KEY(company_id) REFERENCES company(id),\n",
    "    FOREIGN KEY(city_id) REFERENCES city(id),\n",
    "    FOREIGN KEY(country_id) REFERENCES country(id)\n",
    ")\n",
    "'''\n",
    "c.execute(query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.execute('INSERT INTO part (id, name) SELECT part_id, name FROM part_temp')\n",
    "conn.commit()\n",
    "\n",
    "c.execute('INSERT INTO company (id, name) SELECT company_id, name FROM company_temp')\n",
    "conn.commit()\n",
    "\n",
    "c.execute('INSERT INTO country (id, name) SELECT country_id, name FROM country_temp')\n",
    "conn.commit()\n",
    "\n",
    "c.execute('INSERT INTO city (id, name) SELECT city_id, name FROM city_temp')\n",
    "conn.commit()\n",
    "\n",
    "query = '''\n",
    "INSERT INTO trans (part_id, company_id, country_id, city_id, eur, usd, gbp, yen, transaction_date)\n",
    "SELECT part_id, company_id, country_id, city_id, eur, usd, gbp, yen, datetime FROM trans_temp\n",
    "'''\n",
    "c.execute(query)\n",
    "conn.commit()\n",
    "\n",
    "c.execute('DROP TABLE IF EXISTS trans_temp')\n",
    "conn.commit()\n",
    "c.execute('DROP TABLE IF EXISTS part_temp')\n",
    "conn.commit()\n",
    "c.execute('DROP TABLE IF EXISTS company_temp')\n",
    "conn.commit()\n",
    "c.execute('DROP TABLE IF EXISTS city_temp')\n",
    "conn.commit()\n",
    "c.execute('DROP TABLE IF EXISTS country_temp')\n",
    "conn.commit()\n",
    "\n",
    "c.execute('PRAGMA foreign_keys = TRUE') # We actually need to activate foreign key enforcement\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_sql_query('SELECT * FROM trans where eur is null order by transaction_date desc limit 5', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip this cell unless you want to see the foreign key enforcement in action\n",
    "query = '''\n",
    "INSERT INTO trans (part_id, company_id, country_id, city_id, eur, usd, gbp, yen, transaction_date)\n",
    "VALUES (66, 5, 12345678, 11, 784.79, NULL, NULL, NULL, '2016-01-02 00:01:05.000000')\n",
    "'''\n",
    "\n",
    "#query = '''\n",
    "#INSERT INTO country (id, name)\n",
    "#VALUES (12345678, 'Imagination land')\n",
    "#'''\n",
    "\n",
    "c.execute(query)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the connection is closed explicitly by code or implicitly by program exit then any outstanding transaction is rolled back. (The rollback is actually done by the next program to open the database.) If there is no outstanding transaction open then nothing happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Analyzing the data\n",
    "You are now ready to analyze the data. Your goal is to gain some actionable business insights to present to your boss. \n",
    "\n",
    "In this part, you should ask some questions and try to answer them based on the data. You should write SQL queries to retrieve the data. For each question, you should state why it is relevant and what you expect to find.\n",
    "\n",
    "To get you started, you should prepare answers to the following questions. You should add more questions.\n",
    "#### Who are the most profitable clients?\n",
    "Knowing which clients that generate the most revenue for the company will assist your boss in distributing customer service ressources.\n",
    "\n",
    "#### Are there any clients for which profit is declining?\n",
    "Declining profit from a specific client may indicate that the client is disatisfied with the product. Gaining a new client is often much more work than retaining one. Early warnings about declining profit may help your boss fighting customer churn.\n",
    "\n",
    "\n",
    "Remember, you are taking this to your new boss, so think about how you present the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('project1.sqlite')\n",
    "\n",
    "query = '''\n",
    "SELECT\n",
    "    c.name,\n",
    "    t.eur,\n",
    "    t.usd as 'dolla dolla bills y''all',\n",
    "    t.gbp,\n",
    "    t.yen,\n",
    "    t.transaction_date\n",
    "FROM trans t\n",
    "JOIN company c on t.company_id = c.id\n",
    "order by eur desc\n",
    "limit 5\n",
    "'''\n",
    "pd.read_sql_query(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will identify the 10 companies who have the most profit out of all the sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Who are the most profitable clients?\n",
    "most_profitable = '''\n",
    "SELECT c.name, total_count FROM\n",
    "(\n",
    "SELECT c.name, SUM(t.eur) AS total_count\n",
    "FROM trans\n",
    "JOIN company c on t.company_id = c.id\n",
    "GROUP BY company_id\n",
    ")\n",
    "GROUP BY company_id, total_count\n",
    "ORDER BY total_count DESC\n",
    "LIMIT 10\n",
    "'''\n",
    "pd.read_sql_query(most_profitable, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will look if there are any companies who have negative total sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are there any clients for which profit is declining?\n",
    "declining_profit = '''\n",
    "SELECT company_id, total_count FROM\n",
    "(\n",
    "SELECT company_id, SUM(eur) AS total_count FROM trans\n",
    "GROUP BY company_id\n",
    ")\n",
    "WHERE total_count < 0\n",
    "GROUP BY company_id, total_count\n",
    "LIMIT 10\n",
    "'''\n",
    "pd.read_sql_query(declining_profit, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Performance\n",
    "Your boss is very impressed with what you have achieved in less than two weeks, and he would like to take your idea of storing the customer and sales data in a relational database to production. However, John is concerned that the solution will not scale. His experience is telling him that you will see many occurrences of the following queries.\n",
    "\n",
    "- Show all sales to company X between time $t_1$ and time $t_2$\n",
    "- Show the latest X sales in the database\n",
    "- Show total sales per company per day\n",
    "\n",
    "Show that Johns concern is not justified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Show all sales to company X between time $t_1$ and time $t_2$\n",
    "query1 = \"SELECT * FROM trans WHERE company_id = 1 AND transaction_date BETWEEN '2016-04-06' AND '2018-04-06'\"\n",
    "#pd.read_sql_query(query1, conn)\n",
    "\n",
    "# Trying to execute it with params\n",
    "'''\n",
    "id = 1\n",
    "date1 = \"2018-01-01 00:00:00\"\n",
    "date2 = \"2018-01-15 00:00:00\"\n",
    "sql = f\"\"\"\n",
    "    SELECT * FROM trans WHERE company_id = {id} AND transaction_date BETWEEN CONVERT(datetime, {date1}) AND CONVERT(datetime, {date2})\n",
    "    \"\"\"\n",
    "pd.read_sql_query(sql, conn)\n",
    "'''\n",
    "\n",
    "# Show the latest X sales in the database\n",
    "query2 = \"SELECT * FROM trans ORDER BY transaction_date DESC LIMIT 10\"\n",
    "#pd.read_sql_query(query1, conn)\n",
    "\n",
    "# Trying to execute it with params\n",
    "#query2 = \"SELECT * FROM trans ORDER BY transaction_date DESC LIMIT ?\"\n",
    "#c.execute(query2, (10,))\n",
    "#c.fetchall()\n",
    "\n",
    "# Show total sales per company per day\n",
    "query3 = f\"\"\"\n",
    "    SELECT * FROM \n",
    "    (\n",
    "    SELECT company_id, SUM(eur), transaction_date AS total_sales FROM\n",
    "    (\n",
    "    SELECT company_id, eur, transaction_date FROM trans \n",
    "    )\n",
    "    GROUP BY company_id, transaction_date\n",
    "    ORDER BY transaction_date ASC\n",
    "    )\n",
    "    ORDER BY company_id ASC\n",
    "    \"\"\"\n",
    "\n",
    "pd.read_sql_query(query1, conn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>company_id</th>\n",
       "      <th>eur</th>\n",
       "      <th>dolla dolla bills y'all</th>\n",
       "      <th>gbp</th>\n",
       "      <th>yen</th>\n",
       "      <th>transaction_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Avaveo</td>\n",
       "      <td>1</td>\n",
       "      <td>242.46</td>\n",
       "      <td>267.75</td>\n",
       "      <td>212.20</td>\n",
       "      <td>29034.74</td>\n",
       "      <td>2016-02-20 07:11:59.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Avaveo</td>\n",
       "      <td>1</td>\n",
       "      <td>508.39</td>\n",
       "      <td>561.42</td>\n",
       "      <td>444.94</td>\n",
       "      <td>60880.24</td>\n",
       "      <td>2016-02-20 04:46:06.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Avaveo</td>\n",
       "      <td>1</td>\n",
       "      <td>732.99</td>\n",
       "      <td>809.44</td>\n",
       "      <td>641.50</td>\n",
       "      <td>87775.46</td>\n",
       "      <td>2016-02-11 00:43:21.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Avaveo</td>\n",
       "      <td>1</td>\n",
       "      <td>138.40</td>\n",
       "      <td>152.84</td>\n",
       "      <td>121.13</td>\n",
       "      <td>16573.93</td>\n",
       "      <td>2016-02-01 21:45:38.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Avaveo</td>\n",
       "      <td>1</td>\n",
       "      <td>474.35</td>\n",
       "      <td>523.82</td>\n",
       "      <td>415.14</td>\n",
       "      <td>56802.90</td>\n",
       "      <td>2016-01-19 05:50:03.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Avaveo</td>\n",
       "      <td>1</td>\n",
       "      <td>358.46</td>\n",
       "      <td>395.85</td>\n",
       "      <td>313.72</td>\n",
       "      <td>42925.87</td>\n",
       "      <td>2016-01-18 13:54:37.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Avaveo</td>\n",
       "      <td>1</td>\n",
       "      <td>687.63</td>\n",
       "      <td>759.35</td>\n",
       "      <td>601.80</td>\n",
       "      <td>82343.71</td>\n",
       "      <td>2016-01-06 01:04:10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Avaveo</td>\n",
       "      <td>1</td>\n",
       "      <td>1029.93</td>\n",
       "      <td>1137.35</td>\n",
       "      <td>901.37</td>\n",
       "      <td>123333.93</td>\n",
       "      <td>2016-01-03 11:53:31.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name  company_id      eur  dolla dolla bills y'all     gbp        yen  \\\n",
       "0  Avaveo           1   242.46                   267.75  212.20   29034.74   \n",
       "1  Avaveo           1   508.39                   561.42  444.94   60880.24   \n",
       "2  Avaveo           1   732.99                   809.44  641.50   87775.46   \n",
       "3  Avaveo           1   138.40                   152.84  121.13   16573.93   \n",
       "4  Avaveo           1   474.35                   523.82  415.14   56802.90   \n",
       "5  Avaveo           1   358.46                   395.85  313.72   42925.87   \n",
       "6  Avaveo           1   687.63                   759.35  601.80   82343.71   \n",
       "7  Avaveo           1  1029.93                  1137.35  901.37  123333.93   \n",
       "\n",
       "             transaction_date  \n",
       "0  2016-02-20 07:11:59.000000  \n",
       "1  2016-02-20 04:46:06.000000  \n",
       "2  2016-02-11 00:43:21.000000  \n",
       "3  2016-02-01 21:45:38.000000  \n",
       "4  2016-01-19 05:50:03.000000  \n",
       "5  2016-01-18 13:54:37.000000  \n",
       "6  2016-01-06 01:04:10.000000  \n",
       "7  2016-01-03 11:53:31.000000  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conn = sqlite3.connect('project1.sqlite')\n",
    "\n",
    "query = '''\n",
    "SELECT\n",
    "    c.name,\n",
    "    t.company_id,\n",
    "    t.eur,\n",
    "    t.usd as 'dolla dolla bills y''all',\n",
    "    t.gbp,\n",
    "    t.yen,\n",
    "    t.transaction_date\n",
    "FROM trans t\n",
    "JOIN company c on t.company_id = c.id\n",
    "where ? < t.transaction_date and t.transaction_date < ? and t.company_id = ?\n",
    "order by t.transaction_date desc\n",
    "/*limit 5*/\n",
    "'''\n",
    "\n",
    "param = ('2016-01-01 00:00:00.000000', '2016-03-01 00:00:00.000000', '1',)\n",
    "pd.read_sql_query(query, conn, params=param)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
